<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="机器学习, 诗酒趁年华">
    <meta name="description" content="01 什么是人工智能无人驾驶、机器翻译、语音识别、图像识别，这些都是“人工智能”的产物。
“人工智能”Artificial Intelligence），英文缩写为 AI 从字面意思来看，它指的是让机器获得像人一样的智慧。
人工智能（Arti">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>机器学习 | 诗酒趁年华</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

    <script src="/js/FunnyTitle.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">诗酒趁年华</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">诗酒趁年华</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        <!-- 去掉github图标 -->
        <!-- 
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
         -->
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/45.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">机器学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                                <span class="chip bg-color">人工智能</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%8A%80%E6%9C%AF/" class="post-category">
                                技术
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2021-09-13
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    56 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="01-什么是人工智能"><a href="#01-什么是人工智能" class="headerlink" title="01 什么是人工智能"></a>01 什么是人工智能</h2><p>无人驾驶、机器翻译、语音识别、图像识别，这些都是“人工智能”的产物。</p>
<p>“人工智能”Artificial Intelligence），英文缩写为 AI 从字面意思来看，它指的是让机器获得像人一样的智慧。</p>
<p>人工智能（Artificial Intelligence）是计算机科学技术的一个分支，指的是通过机器和计算机来模拟人类智力活动的过程。人工智能并不是人的智能，而是让机器像人一样思考，甚至于超过人类。</p>
<p>其实机器学习是利用大量数据训练出一个最优模型，然后再利用此模型预测出其他数据的一种方法。比如要识别猫、狗照片就要拿它们各自的照片提炼出相应的特征（比如耳朵、脸型、鼻子等），从而训练出一个具有预测能力的模型。</p>
<h3 id="学习形式分类"><a href="#学习形式分类" class="headerlink" title="学习形式分类"></a>学习形式分类</h3><p>机器学习是人工智能的主要表现形式，其学习形式主要分为：有监督学习、无监督学习、半监督学习等，你可以把监督理解为习题的“参考答案”，专业术语叫做<strong>“标记”</strong>。比如有监督学习就是有参考答案的学习，而无监就是无参考答案。</p>
<h3 id="预测结果分类"><a href="#预测结果分类" class="headerlink" title="预测结果分类"></a>预测结果分类</h3><p>比如有监督学习可以划分为：回归问题和分类问题。如果预测结果是离散的，通常为分类问题，而为连续的，则是回归问题。</p>
<p>无监督学习是一种没有“参考答案”的学习形式，它通过在样本之间的比较、计算来实现最终预测输出，比如聚类问题，那什么是“聚类”？其实可以用一个成语表述“物以类聚，人以群分”，将相似的样本聚合在一起后，然后进行分析。</p>
<h2 id="02-机器学习常用术语"><a href="#02-机器学习常用术语" class="headerlink" title="02 机器学习常用术语"></a>02 机器学习常用术语</h2><h3 id="机器学习术语"><a href="#机器学习术语" class="headerlink" title="机器学习术语"></a>机器学习术语</h3><ol>
<li><p><strong>模型</strong></p>
<p>你可以把它看做一个“魔法盒”，你向它许愿（输入数据），它就会帮你实现愿望（输出预测结果）。整个机器学习的过程都将围绕模型展开，训练出一个最优质的“魔法盒”，它可以尽量精准的实现你许的“愿望”，这就是机器学习的目标。</p>
</li>
<li><p><strong>数据集</strong></p>
<p>承载数据的集合，如果说“模型”是“魔法盒”的话，那么数据集就是负责给它充能的“能量电池”，简单地说，如果缺少了数据集，那么模型就没有存在的意义了。数据集可划分为“训练集”和“测试集”，它们分别在机器学习的“训练阶段”和“预测输出阶段”起着重要的作用。</p>
</li>
<li><p><strong>样本&amp;特征</strong></p>
<p>样本指的是数据集中的数据，一条数据被称为“一个样本”，通常情况下，样本会包含多个特征值用来描述数据，比如现在有一组描述人形态的数据“180 70 25”如果单看数据你会非常茫然，但是用“特征”描述后就会变得容易理解，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163429.gif" alt="样本与特征"></p>
<p>可知数据集的构成是“==一行一样本，一列一特征==”。特征值也可以理解为数据的相关性，每一列的数据都与这一列的特征值相关。</p>
</li>
<li><p><strong>向量</strong></p>
<p>向量也称欧几里得向量、几何向量、矢量，指具有大小和方向的量。您可以形象地把它的理解为带箭头的线段。</p>
<p>在机器学习中，模型算法的运算均基于线性代数运算法则，比如行列式、矩阵运算、线性方程等等。向量的计算可采用 NmuPy 来实现，如下所示：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment">#构建向量数组</span>
a<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#加法</span>
a_b<span class="token operator">=</span>a<span class="token operator">+</span>b
<span class="token comment">#数乘</span>
a2<span class="token operator">=</span>a<span class="token operator">*</span><span class="token number">2</span>
b3<span class="token operator">=</span>b<span class="token operator">*</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#减法</span>
b_a<span class="token operator">=</span>a<span class="token operator">-</span>b
<span class="token keyword">print</span><span class="token punctuation">(</span>a_b<span class="token punctuation">,</span>a2<span class="token punctuation">,</span>b3<span class="token punctuation">,</span>b_a<span class="token punctuation">)</span> <span class="token comment"># [2 1] [-2  4] [-9  3] [-4  3]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>简而言之，数据集中的每一个样本都是一条具有向量形式的数据。</p>
</li>
<li><p><strong>矩阵</strong></p>
<p>你可以把矩阵看成由向量组成的二维数组，数据集就是以二维矩阵的形式存储数据的，你可以把它形象的理解为电子表格“一行一样本，一列一特征”表现形式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163509.gif" alt="矩阵电子表"></p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163527.gif" alt="img"></p>
</li>
</ol>
<h3 id="假设函数-amp-损失函数"><a href="#假设函数-amp-损失函数" class="headerlink" title="假设函数&amp;损失函数"></a>假设函数&amp;损失函数</h3><ol>
<li><p><strong>假设函数</strong></p>
<p>假设函数（Hypothesis Function）可表述为<code>y=f(x)</code>其中 x 表示输入数据，而 y 表示输出的预测结果，而这个结果需要不断的优化才会达到预期的结果，否则会与实际值偏差较大。</p>
</li>
<li><p><strong>损失函数</strong></p>
<p>损失函数（Loss Function）又叫目标函数，简写为 L(x)，这里的 x 是假设函数得出的预测结果“y”，如果 L(x) 的返回值越大就表示预测结果与实际偏差越大，越小则证明预测值越来越“逼近”真实值，这才是机器学习最终的目的。因此损失函数就像一个度量尺，让你知道“假设函数”预测结果的优劣，从而做出相应的优化策略。</p>
</li>
<li><p><strong>优化方法</strong></p>
<p>“优化方法”可以理解为假设函数和损失函数之间的沟通桥梁。通过 L(x) 可以得知假设函数输出的预测结果与实际值的偏差值，当该值较大时就需要对其做出相应的调整，这个调整的过程叫做“参数优化”，而如何实现优化呢？这也是机器学习过程中的难点。其实为了解决这一问题，数学家们早就给出了相应的解决方案，比如梯度下降、牛顿方与拟牛顿法、共轭梯度法等等。因此我们要做的就是理解并掌握“科学巨人”留下的理论、方法。</p>
<p><em>对于优化方法的选择，我们要根据具体的应用场景来选择应用哪一种最合适，因为每一种方法都有自己的优劣势，所以只有合适的才是最好的。</em></p>
</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163607.gif" alt="损失函数关系图"></p>
<h3 id="拟合-amp-过拟合-amp-欠拟合"><a href="#拟合-amp-过拟合-amp-欠拟合" class="headerlink" title="拟合&amp;过拟合&amp;欠拟合"></a>拟合&amp;过拟合&amp;欠拟合</h3><ol>
<li><p><strong>拟合</strong></p>
<p>形象地说，“拟合”就是把平面坐标系中一系列散落的点，用一条光滑的曲线连接起来，因此拟合也被称为“曲线拟合”。拟合的曲线一般用函数进行表示，但是由于拟合曲线会存在许多种连接方式，因此就会出现多种拟合函数。通过研究、比较确定一条最佳的“曲线”也是机器学习中一个重要的任务。如下图所示，展示一条拟合曲线（蓝色曲线）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163625.gif" alt="拟合曲线"></p>
</li>
<li><p><strong>过拟合</strong></p>
<p>所谓过拟合，通俗来讲就是模型的泛化能力较差，也就是过拟合的模型在训练样本中表现优越，但是在验证数据以及测试数据集中表现不佳。</p>
<p>过拟合问题在机器学习中经常原道，主要是因为训练时样本过少，特征值过多导致的，</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163646.gif" alt="过拟合模型"></p>
</li>
<li><p><strong>欠拟合</strong></p>
<p>欠拟合（underfitting）恰好与过拟合相反，它指的是“曲线”不能很好的“拟合”数据。在训练和测试阶段，欠拟合模型表现均较差，无法输出理想的预测结果。如下图所示：</p>
<p>造成欠拟合的主要原因是由于没有选择好合适的特征值，比如使用一次函数（y=kx+b）去拟合具有对数特征的散落点（y=log2x）</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163702.gif" alt="欠拟合"></p>
</li>
</ol>
<h2 id="03-Python机器学习环境搭建"><a href="#03-Python机器学习环境搭建" class="headerlink" title="03 Python机器学习环境搭建"></a>03 Python机器学习环境搭建</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>目前而言，在人工智能领域能与 “Python”一较高下的只有 R 语言。不过由于 Python 语言的简洁性、易读性，以及 Python 对科学计算和深度学习框架（Tensorflow、Pytorch 等）的良好支持等，使得 Python 处于远远领先的位置。</p>
<h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><p><code>pip install numpy</code></p>
<p>NumPy（<a target="_blank" rel="noopener" href="https://numpy.org/%EF%BC%89%E5%B1%9E%E4%BA%8E">https://numpy.org/）属于</a> Python 的第三方扩展程序包，它是 Python 科学计算的基础库，提供了多维数组处理、线性代数、傅里叶变换、随机数生成等非常有用的数学工具。</p>
<h3 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h3><p><code>pip install pandas</code></p>
<p>Pandas 属于 Python 第三方数据处理库，它基于 NumPy 构建而来，主要用于数据的处理与分析。我们知道对于机器学习而言数据是尤为重要，如果没有数据就无法训练模型。Pandas 提供了一个简单高效的 DataFrame 对象（类似于电子表格），它能够完成数据的清洗、预处理以及数据可视化工作等。除此之外，Pandas 能够非常轻松地实现对任何文件格式的读写操作，比如 CSV 文件、json 文件、excel 文件。</p>
<h3 id="Scikit-Learn"><a href="#Scikit-Learn" class="headerlink" title="Scikit-Learn"></a>Scikit-Learn</h3><p><code>pip install scikit-learn</code></p>
<p>机器学习中的重要角色 Scikit-Leran（官网：<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/%EF%BC%89%EF%BC%8C%E5%AE%83%E6%98%AF%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E">https://scikit-learn.org/stable/），它是一个基于</a> Python 语言的机器学习算法库。Scikit-Learn 主要用 Python 语言开发，建立在 NumPy、Scipy 与 Matplotlib 之上，它提供了大量机器学习算法接口（API），因此你可以把它看做一本“百科全书”。由于 Scikit-Learn 的存在极大地提高了机器学习的效率，让开发者无须关注数学层面的公式、计算过程，有更多的更多的时间与精力专注于业务层面，从而解决实际的应用问题。</p>
<p>Scikit-Learn 的基本功能主要被分为六大部分：分类，回归，聚类，数据降维，模型选择和数据预处理。</p>
<h2 id="04-线性回归算法"><a href="#04-线性回归算法" class="headerlink" title="04 线性回归算法"></a>04 线性回归算法</h2><p>也就是用线性模型来解决回归问题。</p>
<h3 id="线性回归是什么"><a href="#线性回归是什么" class="headerlink" title="线性回归是什么"></a>线性回归是什么</h3><p>线性回归主要用来解决回归问题，也就是预测连续值的问题。而能满足这样要求的数学模型被称为“回归模型”。最简单的线性回归模型是我们所熟知的一次函数（即 y=kx+b），这种线性函数描述了两个变量之间的关系，其函数图像是一条连续的直线。如下图蓝色直线：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163743.gif" alt="线性函数一次函数"></p>
<p>还有另外一种回归模型，也就是非线性模型(nonlinear model)，它指因变量与自变量之间的关系不能表示为线性对应关系(即不是一条直线)，比如我们所熟知的对数函数、指数函数、二次函数等。</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163759.gif" alt="非线性函数"></p>
<p>我们知道“线性回归”就是利用线性模型来解决“回归问题”，那到底什么是回归问题呢？你可以把它理解为“预测”真实值的过程。</p>
<p>我们反复提起“预测”与“历史数据”，既然是预测，那么就不能说它是 100 % 精确，所以线性回归只是无限地逼近“真实值”，而这个逼近的过程需要大量“历史数据”提供支持。因此线性回归就是利用线性模型来“预测”真实值的过程。</p>
<h3 id="线性回归方程"><a href="#线性回归方程" class="headerlink" title="线性回归方程"></a>线性回归方程</h3><p>线性回归是如何实现预测的呢？其实主要是通过“线性方程”，或叫“回归方程”来实现。</p>
<table>
<thead>
<tr>
<th>输入</th>
<th>输出</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>3</td>
<td>6</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>9</td>
<td>?</td>
</tr>
</tbody></table>
<p>根据上表中的规律预测出 9 所对应的输出值，并写出线性方程。这个示例是不是非常简单，我们很容易想到 9 对应的是“18”，这是一道小学生都能解出来题，但请您不要小看这么一个简单的示例，它同样说明了很多问题。线性方程如下所示：</p>
<p><code>Y=2*X</code></p>
<p>在上述线程方程中<code>2</code>代表<strong>权值参数</strong>，而求这个参数的过程就是“回归”，一旦有了这个参数，再给定输入，做预测就非常容易了。具体的做法就是用回归系数乘以输入值，这样就得到了预测值。上述示例的预测函数（或称假设函数）可记为：</p>
<p><code>y = w1x + b</code></p>
<p>在前面介绍专业术语时，我们提起过“假设函数”，上述函数就是线性模型的“假设函数”。其中 x 表示输入的样本数据，y 表示输出的预测结果，而 w1 指的是线性回归模型的权值参数，b 指的是线性回归模型的“偏差值”。解决线性回归问题的关键就在于求出权值参数、偏差值。</p>
<p><em>权值，可理解为个不同“特征”对于预测结果的重要性。权值系数越大，那么这一项属性值对最终结果的影响就越大。</em></p>
<p>在实际应有中，线性回归模型要更复杂一些，比如要分析实际特征值对结果影响程度的大小，从而调整相应特征值的<strong>回归系数</strong>。下面举一个简单的应用示例：</p>
<p>现在要判断一个西瓜是否是成熟，根据我们的日常经验可从以下几个特征来判断：外表色泽(x)、根蒂(y)、敲声(z)。而以上三个特征所占用的权值参数也不同。如下所示：</p>
<p><code>y = 0.2x1 + 0.5x2 + 0.3 x3 + 1</code></p>
<p>上述表达式可以看出每一个特征值对预测结果的影响程度不同，根蒂是否“枯萎”对结果影响最大，而外表色泽是否鲜亮，敲声是否沉闷则占据次要因素。</p>
<p>当然采集数据的时也会存在一些无用数据，比如西瓜的外形、价格，这些特征不会对预测结果产生影响，因此它们权值参数为“0”。从这个例子可以得出“权值参数”是决定预测结果是否准确的关键因素。</p>
<h3 id="实现预测的流程"><a href="#实现预测的流程" class="headerlink" title="实现预测的流程"></a>实现预测的流程</h3><h4 id="1-数据采集"><a href="#1-数据采集" class="headerlink" title="1) 数据采集"></a>1) 数据采集</h4><p>任何模型的训练都离不开数据，因此收集数据构建数据集是必不可少的环节。比如现在要预测一套房子的售价，那么你必须先要收集周围房屋的售价，这样才能确保你预测的价格不会过高，或过低。如下表所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163826.gif" alt="房屋售价"><br>图3：数据集样本</p>
<p>当然上述样本数量远远不足，如果想要更加准确的预测就要收集更多的数据，至少保证 100 条样本。表格中的最后一栏是“房屋售价”，这是“有监督学习”的典型特点，被称为<strong>“标签”</strong>也就是我们所说的“参考答案”。表格中的面积、数量、距离市中心距离（km），以及是否是学区房，这些都是影响最终预测结果的相关因素，我们称之为“特征”，也叫“属性”。</p>
<p>你可能会认为影响房屋售价的不止这些因素，没错，不过采集数据是一个很繁琐的过程，因此一般情况下，我们只选择与预测结果密切相关的重要“特征”。</p>
<h4 id="2-构建线性回归模型"><a href="#2-构建线性回归模型" class="headerlink" title="2) 构建线性回归模型"></a>2) 构建线性回归模型</h4><p>有了数据以后，下一步要做的就是构建线性回归模型，这也是最为重要的一步，这个过程会涉及到一些数学知识，至于如何构建模型，下一节会做详细介绍。</p>
<p>构建完模型，我们需要对其进行训练，训练的过程就是将表格中的数据以矩阵的形式输入到模型中，模型则通过数学统计方法计算房屋价格与各个特征之间关联关系，也就是“权值参数”。训练完成之后，您就可以对自己的房屋价格进行预测了。首先将数据按照“特征值”依次填好，并输入到模型中，最后模型会输出一个合理的预测结果。示意图如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163845.gif" alt="线性回归预测结果流程图"><br>图4：流程示意图</p>
<p>从上图可知，回归模型承担着非常重要的作用</p>
<h2 id="05-构建线性回归模型"><a href="#05-构建线性回归模型" class="headerlink" title="05 构建线性回归模型"></a>05 构建线性回归模型</h2><h3 id="一次函数"><a href="#一次函数" class="headerlink" title="一次函数"></a>一次函数</h3><p>一次函数就是最简单的“线性模型”，其直线方程表达式为<code>y = kx + b</code>，其中 k 表示<strong>斜率</strong>，b 表示<strong>截距</strong>，x 为<strong>自变量</strong>，y 表示<strong>因变量</strong>。下面展示了 y = 2x + 3 的函数图像：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163911.gif" alt="img"></p>
<p>在机器学习中斜率 k 通常用 w 表示，也就是权重系数，因此“线性方程”通过控制 w 与 b 来实现“直线”与数据点最大程度的“拟合”。如下图（黑色 x 号代表数据样本）所示：</p>
<p><em>线性方程不能完全等同于“直线方程”，因为前者可以描述多维空间内直接，而后者只能描述二维平面内的 x 与 y 的关系。</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163927.gif" alt="线性拟合"></p>
<h3 id="构建线性模型"><a href="#构建线性模型" class="headerlink" title="构建线性模型"></a>构建线性模型</h3><p>从上述函数图像可以看出，直线对数据样本恰好“拟合”。这是最标准的拟合直线，通过它就可以“预测”出小亮明年的年龄了。上述示例就构建了一个简单的的“线性模型”。读到这里你会惊叹“怎么如此简单”，其实线性模型就是这么简单。对于机器学习而言，最关键的就是“学习”，在大量的数据中，通过不断优化参数，找到一条最佳的拟合“直线”，最终预测出一个理想的结果。</p>
<h2 id="06-线性回归：损失函数和假设函数"><a href="#06-线性回归：损失函数和假设函数" class="headerlink" title="06 线性回归：损失函数和假设函数"></a>06 线性回归：损失函数和假设函数</h2><p>数据样本会散落在“线性方程”的周围， 而我们要做就是让线性方程的“直线”尽可能“拟合”周围的数据点。</p>
<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>假设函数是用来预测结果的。</p>
<p>线性方程描绘的是多维空间内的一条“直线”，并且每一个样本都会以向量数组的形式输入到函数中，因此假设函数也会发生一些许变化，函数表达式如下所示：</p>
<p>​    <img src="http://c.biancheng.net/uploads/allimg/210902/1FST251-0.gif" alt="假设函数"> 或 <img src="http://c.biancheng.net/uploads/allimg/210902/1FSW3Y-2.gif" alt="假设函数"></p>
<p>其实它和 Y=wX + b 是类似的，只不过我们这个标量公式换成了向量的形式。如果你已经学习了 《NumPy 教程》，那么这个公司很好理解，<code>Y1</code>仍然代表预测结果， <code>X1</code>表示数据样本， <code>b</code>表示用来调整预测结果的“偏差度量值”，而<code>wT</code>表示权值系数的转置。矩阵相乘法是一个求两个向量<strong>点积</strong>的过程，也就是按位相乘，然后求和，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229163955.gif" alt="矩阵乘法"></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们知道，在线性回归模型中数据样本散落在线性方程的周围，</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164014.gif" alt="线性回归模型"></p>
<p>损失函数就像一个衡量尺，这个函数的返回值越大就表示预测结果与真实值偏差越大。其实计算单个样本的误差值非常简单，只需用预测值减去真实值即可：</p>
<p><code>单样本误差值 = Y1 - Y</code> </p>
<p>但是上述方法只适用于二维平面的直线方程。在线性方程中，要更加复杂、严谨一些，因此我们采用数学中的“均方误差”公式来计算单样本误差：</p>
<p><img src="http://c.biancheng.net/uploads/allimg/210902/1FSV027-4.gif" alt="损失函数"></p>
<p>公式是求“距离”因此要使用平方来消除负数，分母 2 代表样本的数量，这样就求得单样本误差值。当我们知道了单样本误差，那么总样本误差就非常好计算了：</p>
<p><img src="http://c.biancheng.net/uploads/allimg/210902/1FSW109-5.gif" alt="损失函数"></p>
<p>最后，将假设函数带入上述损失函数就会得到一个关于 w 与 b 的损失函数（loss），如下所示：</p>
<p><img src="http://c.biancheng.net/uploads/allimg/210902/1FSQ0B-6.gif" alt="损失函数"></p>
<p>在机器学习中使用损失函数的目的，是为了使用“优化方法”来求得最小的损失值，这样才能使预测值最逼近真实值。</p>
<p>在上述函数中 n、Y、X1 都是已知的，因此只需找到一组 w 与 b 使得上述函数取得最小值即可，这就转变成了数学上二次函数<strong>求极值</strong>的问题，而这个求极值的过程也就我们所说的“优化方法”。关于如何求极值会在下一节做详细介绍。</p>
<h2 id="07-梯度下降求极值"><a href="#07-梯度下降求极值" class="headerlink" title="07 梯度下降求极值"></a>07 梯度下降求极值</h2><p>我们最终的目的要得到一个最佳的“拟合”直线，因此就需要将损失函数的偏差值减到最小，我们把寻找极小值的过程称为“优化方法”，常用的优化方法有很多，比如共轭梯度法、梯度下降法、牛顿法和拟牛顿法。</p>
<h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0/579188?fr=aladdin">导数</a>也叫导函数，或者微商，它是微积分中的重要基础概念，从物理学角度来看，导数是研究物体某一时刻的瞬时速度，而从几何意义上来讲，你可以把它理解为该函数曲线在一点上的切线斜率。</p>
<p>导数有其严格的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0/579188?fr=aladdin">数学定义</a>，它巧妙的利用了极限的思想，也就是无限趋近于 0 的思想。</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164042.png" alt="导数定义"></p>
<p>导数的发明者是伟大的科学家牛顿与布莱尼茨，它是微积分的一个重要的支柱。在机器学习中，我们只需会用前辈科学家们留下来的知识就行了，比如熟悉常见的导函数公式，以下列举了常用的导数公式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164058.gif" alt="导数公式"></p>
<h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>当求 x 的偏导时就要把 y 当做常数项来对待，而当求 y 的偏导时就要把 x 当做常数项对待。关于偏导数还会涉及到高阶偏，如果感兴趣的话可以点击<a target="_blank" rel="noopener" href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530802.htm">了解一下</a>。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降是机器学习中常用的一种优化方法，主要用来解决求极小值的问题，某个函数在某点的梯度指向该函数取得最大值的方向，那么它的反方向自然就是取得最小值的方向。在解决线性回归和 Logistic（逻辑） 回归问题时，梯度下降方法有着广泛的应用。</p>
<p>梯度是微积分学的术语，它本质上是一个向量，表示函数在某一点处的方向导数上沿着特定的方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大。梯度下降法的计算过程就是沿梯度方向求解极小值，当然你也可以沿梯度上升的方向求解极大值。</p>
<p>那么如何能够更好的理解“梯度下降”呢？如果不考虑其他外在因素，其实你可以把它想象成“下山”的场景，如何从一个高山上以最快的时间走到山脚下呢？其实很简单，以你所在的当前位置为基准，寻找该位置最陡峭的地方，然后沿着此方向向下走，并且每走一段距离，都要寻找当前位置“最陡峭的地方”，反复采用上述方法，最终就能以最快的时间抵达山脚下。</p>
<p>在这个下山的过程中，“寻找所处位置最陡峭的地方，并沿此位置向下走”最为关键，如果把这个做法对应到函数中，就是找到“给定点的梯度”而梯度的方向就是函数值变化最快的方向。</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164117.gif" alt="梯度下降"><br>图1：示意图</p>
<p>从上述描述中，你可能感觉到平淡无奇，其实每一个词语都蕴含着数学知识，比如“以当前所在位置为基准，找到最陡峭的地方”从数学角度来讲就是找到所在点的“切线”方向，也就是对这点“求导”，然后循着切线轨迹点反复使用此方法，就可以到达极小值点。</p>
<p>在《<a target="_blank" rel="noopener" href="http://c.biancheng.net/ml_alg/hypothesis-loss.html">线性回归：损失函数和假设函数</a>》一节，我们讲解了线性回归的损失函数，而梯度下降作为一种优化方法，其目的是要使得损失值最小。因此“梯度下降”就需要控制损失函数的<code>w</code>和<code>b</code>参数来找到最小值。比如控制 w 就会得到如下方法：</p>
<p>w新=w旧 - 学习率 * 损失值</p>
<p>通过梯度下降计算极小值时，需要对损失函数的<code>w</code>求偏导求得，这个偏导也就是“梯度”，通过损失值来调节<code>w</code>，不断缩小损失值直到最小，这也正是梯度下降的得名来由。</p>
<p>“学习率”是一个由外部输入的参数，被称为“超参数”，可以形象地把它理解为下山时走的“步长”大小，想要 w 多调整一点，就把学习率调高一点。不过学习率也不是越高越好，过高的学习率可能导致调整幅度过大，导致无法求得真正的最小值。当损失函数取得极小值时，此时的参数值被称为“最优参数”。因此，在机器学习中最重要的一点就是寻找“最优参数”。</p>
<p>梯度下降是个大家族，它有很多成员，比如批量梯度下降（BGD）、随机梯度下降（SGD）、小批量梯度下降（MBGD），其中批量梯度下降是最常用的，相关内容后续会详细介绍。 </p>
<h2 id="08-sklearn应用线性回归算法"><a href="#08-sklearn应用线性回归算法" class="headerlink" title="08 sklearn应用线性回归算法"></a>08 sklearn应用线性回归算法</h2><p>下面介绍 sklearn 中常用的算法库：</p>
<ul>
<li>linear_model：线性模型算法族库，包含了线性回归算法，以及 Logistic 回归算法，它们都是基于线性模型。</li>
<li>naiv_bayes：朴素贝叶斯模型算法库。</li>
<li>tree：决策树模型算法库。</li>
<li>svm：支持向量机模型算法库。</li>
<li>neural_network：神经网络模型算法库。</li>
<li>neightbors：最近邻算法模型库。</li>
</ul>
<h3 id="实现线性回归算法"><a href="#实现线性回归算法" class="headerlink" title="实现线性回归算法"></a>实现线性回归算法</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 下面我们是基于 sklearn 实现线性回归算法，大概可以分为三步，首先从 sklearn 库中导入线性模型中的线性回归算法，如下所示：</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> linear_model
<span class="token comment"># 其次训练线性回归模型。使用  fit() 喂入训练数据，如下所示：</span>
model <span class="token operator">=</span> linear_model<span class="token punctuation">.</span>LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token comment"># 最后一步就是对训练好的模型进行预测。调用 predict() 预测输出结果， “x_”为输入测试数据，如下所示：</span>
model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>数据集的整理也是一门专业的知识，会涉及到数据的收集、清洗，也就是预处理的过程，比如均值移除、归一化等操作，</p>
<ol>
<li><p>准备数据</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用numpy准备数据集</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># 准备自变量x,-3到3的区间均分间隔30份数</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6.40</span><span class="token punctuation">)</span>
<span class="token comment">#准备因变量y，这一个关于x的假设函数</span>
y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">2</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>实现算法</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#使用matplotlib绘制图像，使用numpy准备数据集</span>
<span class="token keyword">import</span>  matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> linear_model
<span class="token comment">#准备自变量x，生成数据集，3到6的区间均分间隔30份数</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6.40</span><span class="token punctuation">)</span>
<span class="token comment">#准备因变量y，这一个关于x的假设函数</span>
y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">2</span>
<span class="token comment">#由于fit 需要传入二维矩阵数据，因此需要处理x，y的数据格式,将每个样本信息单独作为矩阵的一行</span>
x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> x<span class="token punctuation">]</span>
y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> y<span class="token punctuation">]</span>
<span class="token comment"># 构建线性回归模型</span>
model<span class="token operator">=</span>linear_model<span class="token punctuation">.</span>LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 训练模型，"喂入"数据</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span class="token comment"># 准备测试数据 x_，这里准备了三组，如下：</span>
x_<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># 打印预测结果</span>
y_<span class="token operator">=</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_<span class="token punctuation">)</span>
<span class="token comment">#查看w和b的</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w值为:"</span><span class="token punctuation">,</span>model<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"b截距值为:"</span><span class="token punctuation">,</span>model<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>
<span class="token comment">#数据集绘制,散点图，图像满足函假设函数图像</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<h3 id="线性回归步骤"><a href="#线性回归步骤" class="headerlink" title="线性回归步骤"></a>线性回归步骤</h3><p>线性回归适用于有监督学习的回归问题，首先在构建线性模型前，需要准备好待输入的数据集，数据集按照需要可划分为训练集和测试集，使用训练集中的向量 X 与向量 Y 进行模型的训练，其中向量 Y 表示对应 X 的结果数值(也就是“参考答案”)；而输出时需要使用测试集，输入测试 X 向量输出预测结果向量 Y。</p>
<p>其实线性回归主要解决了以下三个问题：</p>
<ul>
<li>第一，为假设函数设定了参数 w，通过假设函数画出线性“拟合”直线。</li>
<li>第二，将预测值带入损失函数，计算出一个损失值。</li>
<li>第三，通过得到的损失值，利用梯度下降等优化方法，不断调整 w 参数，使得损失值取得最小值。我们把这个优化参数值的过程叫做“线性回归”的学习过程。</li>
</ul>
<h2 id="09-Logistic回归算法（分类问题）"><a href="#09-Logistic回归算法（分类问题）" class="headerlink" title="09 Logistic回归算法（分类问题）"></a>09 Logistic回归算法（分类问题）</h2><p>我们知道有监督学习分为“回归问题”和“分类问题”，</p>
<h3 id="什么是分类问题？"><a href="#什么是分类问题？" class="headerlink" title="什么是分类问题？"></a>什么是分类问题？</h3><p>其实想要理解“分类”问题非常的简单，我们不妨拿最简单的“垃圾分类处理”的过程来认识一下这个词。</p>
<p>“可回收”与“不可回收”是两种预测分类，而小明是主观判断的个体，他通过自己日常接触的知识对“垃圾种类”做出判断，我们把这个程称作“模型训练”，只有通过“训练”才可以更加准确地判断“垃圾”的种类。小明进行了两次投放动作，每一次投放都要对“垃圾”种类做出预先判断，最终决定投放到哪个垃圾桶内。这就是根据模型训练的结果进行预测的整个过程。</p>
<p>下面对上述过程做简单总结：</p>
<ul>
<li>类别标签：“可回收”与“不可回收”。</li>
<li>模型训练：以小明为主体，把他所接受的知识、经验做为模型训练的参照。</li>
<li>预测：投放垃圾的结果，预测分类是否正确。并输出预测结果。</li>
</ul>
<h3 id="Logistic回归算法"><a href="#Logistic回归算法" class="headerlink" title="Logistic回归算法"></a>Logistic回归算法</h3><p>Logistic 回归算法，又叫做逻辑回归算法，或者 LR 算法（Logistic Regression）。分类问题同样也可以基于“线性模型”构建。“线性模型”最大的特点就是“直来直去”不会打弯，而我们知道，分类问题的预测结果是“离散的”，即对输出数据的类别做判断。比如将类别预设条件分为“0”类和“1”类（或者“是”或者“否”）那么图像只会在 “0”和“1”之间上下起伏，</p>
<p>在机器学习中，Logistic 函数通常用来解决二元分类问题，也就是涉及两个预设类别的问题，而当类别数量超过两个时就需要使用 Softmax 函数来解决。</p>
<p>19 世纪统计学家<strong>皮埃尔·弗朗索瓦·韦吕勒</strong>发明了 Logistic 函数，该函数的叫法有很多，比如在神经网络算法中被称为 <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407?fr=aladdin"><strong>Sigmoid 函数</strong></a>，也有人称它为 <strong>Logistic 曲线。</strong>其函数图像如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164154.gif" alt="logistic函数"></p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164210.gif" alt="logistic函数"></p>
<p>Logistic 函数也称为 S 型生长曲线，取值范围为 (0,1)，它可以将一个实数映射到 (0,1) 的区间，非常适合做二元分类。当 z=0 时，该函数的取值为 0.5，随着 z 的增大，对应的函数值将逼近于 1；而随着 z 的减小，其函数值将逼近于 0。</p>
<p>对于 Logistic 函数而言，坐标轴 0 是一个有着特殊意义坐标，越靠近 0 和越远离 0 会出现两种截然不同的情况：任何大于 0.5 的数据都会被划分到 “1”类中；而小于 0.5 会被归如到 “0”类。因此你可以把 Logistic 看做解决二分类问题的分类器。如果想要 Logistic 分类器预测准确，那么 x 的取值距离 0 越远越好，这样结果值才能无限逼近于 0 或者 1。</p>
<h2 id="10-数学解析Logistic回归算法"><a href="#10-数学解析Logistic回归算法" class="headerlink" title="10 数学解析Logistic回归算法"></a>10 数学解析Logistic回归算法</h2><h3 id="分类数据表示形式"><a href="#分类数据表示形式" class="headerlink" title="分类数据表示形式"></a>分类数据表示形式</h3><ol>
<li><p>向量形式</p>
<p>在机器学习中，向量形式是应用最多的形式，使用向量中的元素按顺序代表“类别”。</p>
</li>
<li><p>数字形式</p>
<p>数字形式是一种最简单的分类方式，我们可以用 0 代表“负类”，而用“1”代表正类，数字并不局限于“0”和“1”。</p>
</li>
<li><p>概率形式</p>
<p><code>[0.8,0.1,0.1]</code>，从输出结果不难看出，该样本属于 a 类的概率最大，因此我们可以认定该样本从属于 a 类。</p>
</li>
</ol>
<h3 id="Logistic函数数学解析"><a href="#Logistic函数数学解析" class="headerlink" title="Logistic函数数学解析"></a>Logistic函数数学解析</h3><ol>
<li><p>假设函数</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164240.gif" alt="逻辑回归假设函数"></p>
<p>上述公式和 Logistic 函数基本一致，只不过我们它换成了关于<code>x</code>的表达式，并将幂指数<code>x</code>换成了 “线性函数”表达式。H(x) 的函数图像呈现 S 形分布，从而能够预测出离散的输出结果。</p>
</li>
<li><p>损失函数</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164306.png" alt="损失函数"></p>
</li>
<li><p>优化方法</p>
</li>
</ol>
<h3 id="梯度上升优化方法"><a href="#梯度上升优化方法" class="headerlink" title="梯度上升优化方法"></a>梯度上升优化方法</h3><p>梯度上升基于的思想是：要找到某函数的最大值，最好的发放是沿着该函数的梯度方向寻找，如果把梯度记为<code>▽</code>，那么关于 f(x,y) 有以下表达式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164328.gif" alt="img"></p>
<h2 id="11-sklearn应用Logistic回归算法"><a href="#11-sklearn应用Logistic回归算法" class="headerlink" title="11 sklearn应用Logistic回归算法"></a>11 sklearn应用Logistic回归算法</h2><h3 id="什么是范数？"><a href="#什么是范数？" class="headerlink" title="什么是范数？"></a>什么是范数？</h3><p>范数又称为“正则项”，常见的范数主要分为两种：L1 和 L2。</p>
<ol>
<li><p>L1范数，绝对值的和</p>
<p>L1 范数非常容易理解，它表示向量中每个元素绝对值的和，</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164408.gif" alt="L1范数"></p>
</li>
<li><p>L2范数，平方 求和 开根</p>
<p>L2 范数出现的频率更高，表示向量中每个元素的平方和的平方根。</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164424.gif" alt="L2范数表达式"></p>
</li>
</ol>
<h3 id="回归类算法"><a href="#回归类算法" class="headerlink" title="回归类算法"></a>回归类算法</h3><ol>
<li><p>Ridge类</p>
<p>Ridge 回归算法，又称“岭回归算法”主要用于预测回归问题，是在线性回归的基础上添加了 L2 正则项，使得权重 w 的分布更加均匀，其损失函数如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164441.png" alt="损失函数"></p>
</li>
<li><p>Lasso类</p>
<p>Lasso 回归算法：我们知道，常用的正则项有 L1 和 L2，而使用了 L1 正则项的线性回归是 Lasso 回归算法，它可以预测回归问题，其损失函数的表达式如下（求最小损失值）： </p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164456.png" alt="损失函数"></p>
</li>
</ol>
<h3 id="实现Logistic回归"><a href="#实现Logistic回归" class="headerlink" title="实现Logistic回归"></a>实现Logistic回归</h3><p>鸢尾花数据集对 Logistic 回归算法</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#logistic算法</span>
<span class="token comment">#从 scikit-learn库导入线性模型中的logistic回归算法</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token comment">#导入sklearn 中的自带数据集 鸢尾花数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
<span class="token comment"># skleran 提供的分割数据集的方法</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token comment">#载入鸢尾花数据集</span>
iris_dataset<span class="token operator">=</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># data 数组的每一行对应一朵花，列代表每朵花的四个测量数据，分别是：花瓣的长度，宽度，花萼的长度、宽度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"data数组类型: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>iris_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 前五朵花的数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"前五朵花数据:\n&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>iris_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#分割数据集训练集，测试集</span>
X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>Y_train<span class="token punctuation">,</span>Y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>iris_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>iris_dataset<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment">#训练模型</span>
<span class="token comment">#设置最大迭代次数为3000，默认为1000.不更改会出现警告提示</span>
log_reg <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span>max_iter<span class="token operator">=</span><span class="token number">3000</span><span class="token punctuation">)</span>
<span class="token comment">#给模型喂入数据</span>
clm<span class="token operator">=</span>log_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
<span class="token comment">#使用模型对测试集分类预测,并打印分类结果</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>clm<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#最后使用性能评估器，测试模型优良，用测试集对模型进行评分</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>clm<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>Y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>scikit-learn 中的 train_test_split 函数可以打乱数据集，并对其进行拆分。该函数默认将 75% 的行数据及对应标签作为训练集，另外 25% 数据作为测试集。</p>
<p>最后，我们对 Logistic 算法做一下简单总结：首先 Logistic 算法适用于分类问题，该算法在处理二分类问题上表现优越，但在多分类（二个以上）问题上容易出现欠拟合。Logistic 算法除了适用于回归分类问题，还可以作为神经网络算法的激活函数（即 Sigmoid 函数）。</p>
<h2 id="12-KNN最邻近分类算法"><a href="#12-KNN最邻近分类算法" class="headerlink" title="12 KNN最邻近分类算法"></a>12 KNN最邻近分类算法</h2><blockquote>
<p>K 最近邻分类算法，简称 KNN（K-Nearest-Neighbor），它是有监督学习分类算法的一种。所谓 K 近邻，就是 K 个最近的邻居。比如对一个样本数据进行分类，我们可以用与它最邻近的 K 个样本来表示它，这与俗语“近朱者赤，近墨者黑”是一个道理。</p>
<p>在学习 KNN 算法的过程中，你需要牢记两个关键词，一个是“<strong>少数服从多数</strong>”，另一个是“<strong>距离</strong>”，它们是实现 KNN 算法的核心知识。</p>
</blockquote>
<h3 id="KNN算法原理"><a href="#KNN算法原理" class="headerlink" title="KNN算法原理"></a>KNN算法原理</h3><p>为了判断未知样本的类别，以所有已知类别的样本作为参照来计算未知样本与所有已知样本的距离，然后从中选取与未知样本距离最近的 K 个已知样本，并根据少数服从多数的投票法则（majority-voting），将未知样本与 K 个最邻近样本中所属类别占比较多的归为一类。这就是 KNN 算法基本原理。</p>
<blockquote>
<p>在 scikit-learn 中 KNN 算法的 K 值是通过 n_neighbors 参数来调节的，默认值是 5。</p>
</blockquote>
<p>KNN 算法原理：如果一个样本在特征空间中存在 K 个与其相邻的的样本，其中某一类别的样本数目较多，则待预测样本就属于这一类，并具有这个类别相关特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p>KNN 算法简单易于理解，无须估计参数，与训练模型，适合于解决多分类问题。但它的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有很能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数，而此时只依照数量的多少去预测未知样本的类型，就会可能增加预测错误概率。此时，我们就可以采用对样本取<strong>“权值”</strong>的方法来改进。</p>
<h3 id="KNN算法流程"><a href="#KNN算法流程" class="headerlink" title="KNN算法流程"></a>KNN算法流程</h3><ol>
<li>准备数据，对数据进行预处理 。</li>
<li>计算测试样本点（也就是待分类点）到其他每个样本点的距离（选定度量距离的方法）。</li>
<li>对每个距离进行排序，然后选择出距离最小的 K 个点。</li>
<li>对 K 个点所属的类别进行比较，按照少数服从多数的原则（多数表决思想），将测试样本点归入到 K 个点中占比最高的一类中。</li>
</ol>
<p>注意：在机器学习中有多种不同的距离公式，下面以计算二维空间 A(x,y)，B(x1,y1) 两点间的距离为例进行说明，下图展示了如何计算欧式距离和曼哈顿街区距离。（PS:要理会名字，名字都是纸老虎）如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164522.gif" alt="常见距离公式"></p>
<p>注意：除上述距离外，还有汉明距离、余弦距离、切比雪夫距离、马氏距离等。在 KNN 算法中较为常用的距离公式是“欧氏距离”。</p>
<h3 id="KNN预测分类"><a href="#KNN预测分类" class="headerlink" title="KNN预测分类"></a>KNN预测分类</h3><p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164542.gif" alt="KNN算法核心"></p>
<p>如图所示，有三角形和菱形两个类别，而“灰色圆”是一个未知类别，现在通过 KNN 算法判断“灰色圆”属于哪一类。如果当 K 的取值为 3 时，按照前面讲述的知识，距离最近且少数服从多数，那“灰色圆”属于菱形类，而当 K= 6 时，按照上述规则继续判断，则“灰色圆”属于三角形类。</p>
<blockquote>
<p>KNN 分类算法适用于多分类问题、OCR光学模式识别、文本分类等领域</p>
</blockquote>
<h2 id="13-sklearn实现KNN分类算法"><a href="#13-sklearn实现KNN分类算法" class="headerlink" title="13 sklearn实现KNN分类算法"></a>13 sklearn实现KNN分类算法</h2><p>Pyhthon Sklearn 机器学习库提供了 neighbors 模块，该模块下提供了 KNN 算法的常用方法</p>
<table>
<thead>
<tr>
<th>类方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>KNeighborsClassifier</td>
<td>KNN 算法解决分类问题</td>
</tr>
<tr>
<td>KNeighborsRegressor</td>
<td>KNN 算法解决回归问题</td>
</tr>
<tr>
<td>RadiusNeighborsClassifier</td>
<td>基于半径来查找最近邻的分类算法</td>
</tr>
<tr>
<td>NearestNeighbors</td>
<td>基于无监督学习实现KNN算法</td>
</tr>
<tr>
<td>KDTree</td>
<td>无监督学习下基于 KDTree 来查找最近邻的分类算法</td>
</tr>
<tr>
<td>BallTree</td>
<td>无监督学习下基于 BallTree 来查找最近邻的分类算法</td>
</tr>
</tbody></table>
<p>对 Sklearn 自带的“红酒数据集”进行 KNN 算法分类预测。最终实现向训练好的模型喂入数据，输出相应的红酒类别</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#加载红酒数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine
<span class="token comment">#KNN分类算法</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> KNeighborsClassifier
<span class="token comment">#分割训练集与测试集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token comment">#导入numpy</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment">#加载数据集</span>
wine_dataset<span class="token operator">=</span>load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#查看数据集对应的键</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"红酒数据集的键:\n&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>wine_dataset<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"数据集描述:\n&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># data 为数据集数据;target 为样本标签</span>
<span class="token comment">#分割数据集，比例为 训练集：测试集 = 8:2</span>
X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment">#构建knn分类模型，并指定 k 值</span>
KNN<span class="token operator">=</span>KNeighborsClassifier<span class="token punctuation">(</span>n_neighbors<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment">#使用训练集训练模型</span>
KNN<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
<span class="token comment">#评估模型的得分</span>
score<span class="token operator">=</span>KNN<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>score<span class="token punctuation">)</span>
<span class="token comment">#给出一组数据对酒进行分类</span>
X_wine_test<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11.8</span><span class="token punctuation">,</span><span class="token number">4.39</span><span class="token punctuation">,</span><span class="token number">2.39</span><span class="token punctuation">,</span><span class="token number">29</span><span class="token punctuation">,</span><span class="token number">82</span><span class="token punctuation">,</span><span class="token number">2.86</span><span class="token punctuation">,</span><span class="token number">3.53</span><span class="token punctuation">,</span><span class="token number">0.21</span><span class="token punctuation">,</span><span class="token number">2.85</span><span class="token punctuation">,</span><span class="token number">2.8</span><span class="token punctuation">,</span><span class="token number">.75</span><span class="token punctuation">,</span><span class="token number">3.78</span><span class="token punctuation">,</span><span class="token number">490</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
predict_result<span class="token operator">=</span>KNN<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_wine_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>predict_result<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"分类结果：&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'target_names'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>predict_result<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="14-通俗地理解贝叶斯公式（定理）"><a href="#14-通俗地理解贝叶斯公式（定理）" class="headerlink" title="14 通俗地理解贝叶斯公式（定理）"></a>14 通俗地理解贝叶斯公式（定理）</h2><p>朴素贝叶斯（Naive Bayesian algorithm）是有监督学习的一种分类算法，它基于“贝叶斯定理”实现，</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理的发明者 <strong>托马斯·贝叶斯</strong> 提出了一个很有意思的假设：“如果一个袋子中共有 10 个球，分别是黑球和白球，但是我们不知道它们之间的比例是怎么样的，现在，仅通过摸出的球的颜色，是否能判断出袋子里面黑白球的比例？”</p>
<p>在统计学中有两个较大的分支：一个是“频率”，另一个便是“贝叶斯”，它们都有各自庞大的知识体系，而“贝叶斯”主要利用了“相关性”一词。下面以通俗易懂的方式描述一下“贝叶斯定理”：通常，事件 A 在事件 B 发生的条件下与事件 B 在事件 A 发生的条件下，它们两者的概率并不相同，但是它们两者之间存在一定的相关性，并具有以下公式（称之为“贝叶斯公式”）：</p>
<p><img src="http://c.biancheng.net/uploads/allimg/210902/1GUU234-0.gif" alt="贝叶斯公式"></p>
<p>贝叶斯公式可以预测事件发生的概率，两个本来相互独立的事件，发生了某种“相关性”，此时就可以通过“贝叶斯公式”实现预测。</p>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>贝叶斯公式的核心是“条件概率”，譬如 P(B|A)，就表示当 A 发生时，B 发生的概率，如果P(B|A)的值越大，说明一旦发生了 A，B 就越可能发生。两者可能存在较高的相关性。</p>
<h3 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h3><p>在<strong>贝叶斯</strong>看来，世界并非静止不动的，而是动态和相对的，他希望利用已知经验来进行判断，那么如何用经验进行判断呢？这里就必须要提到“先验”和“后验”这两个词语。我们先讲解“先验”，其实“先验”就相当于“未卜先知”，在事情即将发生之前，做一个概率预判。比如从远处驶来了一辆车，是轿车的概率是 45%，是货车的概率是 35%，是大客车的概率是 20%，在你没有看清之前基本靠猜，此时，我们把这个概率就叫做“先验概率”。</p>
<h3 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h3><blockquote>
<p>在某些已知条件下， 概率不再是猜出来的了</p>
</blockquote>
<p>在理解了“先验概率”的基础上，我们来研究一下什么是“后验概率？”</p>
<p>我们知道每一个事物都有自己的特征，比如前面所说的轿车、货车、客车，它们都有着各自不同的特征，距离过远的时候，我们无法用肉眼分辨，而当距离达到一定范围内就可以根据各自的特征再次做出概率预判，这就是后验概率。比如轿车的速度相比于另外两者更快可以记做 P(轿车|速度快) = 55%，而客车体型可能更大，可以记做 P(客车|体型大) = 35%。</p>
<p>如果用<strong>条件概率</strong>来表述 P(体型大|客车)=35%，这种通过“车辆类别”推算出“类别特征”发生的的概率的方法叫作“似然度”。这里的似然就是“可能性”的意思。</p>
<h3 id="朴素-贝叶斯"><a href="#朴素-贝叶斯" class="headerlink" title="朴素+贝叶斯"></a>朴素+贝叶斯</h3><blockquote>
<p>纯粹的 理想情况下的 条件概率</p>
</blockquote>
<p>实际上贝叶斯定理就是求解后验概率的过程，而核心方法是通过似然度预测后验概率，通过不断提高似然度，自然也就达到了提高后验概率的目的。</p>
<p>朴素贝叶斯是一种简单的贝叶斯算法，因为贝叶斯定理涉及到了概率学、统计学，其应用相对复杂，因此我们只能以简单的方式使用它，比如天真的认为，所有事物之间的特征都是相互独立的，彼此互不影响。</p>
<h2 id="15-朴素贝叶斯分类算法原理"><a href="#15-朴素贝叶斯分类算法原理" class="headerlink" title="15 朴素贝叶斯分类算法原理"></a>15 朴素贝叶斯分类算法原理</h2><p><strong>多特征分类</strong>：比如现在有 A1 和 A2 两个类，其中 A1 具有 b、c 两个特征，A2 具有 b、d 两个 特征，如果是你会怎么区分这两个类呢？很简单看看是存在 c ，存在的就是 A1，反之则是 A2。但是现实的情况要复杂的多，比如 100 个 A1样本中有 80% 的样本具有特征 c，而且剩余的 20% 具有了特征 d，那么要怎么对它们分类呢？其实只要多加判断还是可以分清，不过要是纯手工分类，那就恐怕得不偿失了。</p>
<h3 id="多特征分类问题"><a href="#多特征分类问题" class="headerlink" title="多特征分类问题"></a>多特征分类问题</h3><p>下面我们使统计学的相关知识解决上述分类问题，分类问题的样本数据大致如下所示：</p>
<pre class="line-numbers language-none"><code class="language-none">[特征 X1 的值,特征 X2 的值,特征 X3 的值,......,类别 A1]
[特征 X1 的值,特征 X2 的值,特征 X3 的值,......,类别 A2]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>解决思路：</strong>这里我们先简单的采用 1 和 0 代表特征值的有无，比如当 X1 的特征值等于 1 时，则该样本属于 A1 的类别概率；特征值 X2 值为 1 时，该样本属于类别 A1 的类别的概率。依次类推，然后最终算出该样本对于各个类别的概率值，哪个<strong>概率值最大</strong>就可能是哪个类。</p>
<p>上述思路就是贝叶斯定理的典型应用，如果使用条件概率表达，如下所示：</p>
<p><code>P(类别A1|特征X1，特征X2，特征X3，…)</code></p>
<h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><h3 id="朴素贝叶斯优化方法"><a href="#朴素贝叶斯优化方法" class="headerlink" title="朴素贝叶斯优化方法"></a>朴素贝叶斯优化方法</h3><h2 id="16-sklearn应用朴素贝叶斯算法"><a href="#16-sklearn应用朴素贝叶斯算法" class="headerlink" title="16 sklearn应用朴素贝叶斯算法"></a>16 sklearn应用朴素贝叶斯算法</h2><h3 id="简单应用案例"><a href="#简单应用案例" class="headerlink" title="简单应用案例"></a>简单应用案例</h3><p>假设一个学校有 45% 的男生和 55% 的女生，学校规定不能穿奇装异服，男生的裤子只能穿长筒裤，而女生可以穿裙子或者长筒裤，已知该学校穿长筒裤的女生和穿裙子的女生数量相等，所有男生都必须穿长筒裤，请问如果你从远处看到一个穿裤子的学生，那么这个学生是女生的概率是多少？</p>
<p>看完上述问题，你是不是已经很快的计算出了结果呢？还是丈二和尚，摸不到头脑呢？下面我们一起来分析一下，我们根据贝叶斯公式，列出要用到的事件概率：</p>
<pre class="line-numbers language-none"><code class="language-none">学校女生的概率：P(女生)&#x3D; 0.55
女生中穿裤子的概率：P(裤子|女)&#x3D; 0.5
学校中穿裤子的概率：P(裤子)&#x3D; 0.45 + 0.275&#x3D; 0.725<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>知道了上述概率，下面使用贝叶斯公式求解 P(女生|裤子) 的概率：</p>
<pre class="line-numbers language-none"><code class="language-none">P(女|裤子) &#x3D; P(裤子|女生) * P(女生) &#x2F; P(裤子) &#x3D; 0.5 * 0.55 &#x2F; 0.725 &#x3D; 0.379<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>利用上述公式就计算除了后验概率 P(女生|裤子) 的概率，这里的 P(女生) 和 P(裤子)叫做先验概率，而 P(裤子|女生) 就是我们经常提起的条件概率“似然度”。</p>
<h3 id="sklearn实现朴素贝叶斯"><a href="#sklearn实现朴素贝叶斯" class="headerlink" title="sklearn实现朴素贝叶斯"></a>sklearn实现朴素贝叶斯</h3><p>在 sklearn 库中，基于贝叶斯定理的算法集中在 sklearn.naive_bayes 包中，根据对“似然度 P(xi|y)”计算方法的不同，我们将朴素贝叶斯大致分为三种：多项式朴素贝叶斯（MultinomialNB）、伯努利分布朴素贝叶斯（BernoulliNB)、高斯分布朴素贝叶斯（GaussianNB）。另外一点要牢记，朴素贝叶斯算法的实现是基于假设而来，在朴素贝叶斯看来，特征之间是相互独立的，互不影响的。</p>
<p>高斯朴素贝叶斯适用于特征呈正态分布的，多项式贝叶斯适用于特征是多项式分布的，伯努利贝叶斯适用于二项分布。</p>
<h4 id="1-算法使用流程"><a href="#1-算法使用流程" class="headerlink" title="1) 算法使用流程"></a>1) 算法使用流程</h4><p>使用朴素贝叶斯算法，具体分为三步：</p>
<ul>
<li>统计样本数，即统计先验概率 P(y) 和 似然度 P(x|y)。</li>
<li>根据待测样本所包含的特征，对不同类分别进行后验概率计算。</li>
<li>比较 y1，y2，…yn 的后验概率，哪个的概率值最大就将其作为预测输出。</li>
</ul>
<h4 id="2-朴素贝叶斯算法应用"><a href="#2-朴素贝叶斯算法应用" class="headerlink" title="2) 朴素贝叶斯算法应用"></a>2) 朴素贝叶斯算法应用</h4><p>下面通过鸢尾花数据集对朴素贝叶斯分类算法进行简单讲解。如下所示：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#鸢尾花数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
<span class="token comment">#导入朴素贝叶斯模型，这里选用高斯分类器</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>naive_bayes <span class="token keyword">import</span> GaussianNB
<span class="token comment">#载入数据集</span>
X<span class="token punctuation">,</span>y<span class="token operator">=</span>load_iris<span class="token punctuation">(</span>return_X_y<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
bayes_modle<span class="token operator">=</span>GaussianNB<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#训练数据</span>
bayes_modle<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span class="token comment">#使用模型进行分类预测</span>
result<span class="token operator">=</span>bayes_modle<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
<span class="token comment">#对模型评分</span>
model_score<span class="token operator">=</span>bayes_modle<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model_score<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="17-决策树分类算法（if-else原理）"><a href="#17-决策树分类算法（if-else原理）" class="headerlink" title="17 决策树分类算法（if-else原理）"></a>17 决策树分类算法（if-else原理）</h2><h3 id="if-else原理"><a href="#if-else原理" class="headerlink" title="if-else原理"></a>if-else原理</h3><p>下面我看一个简单的应用示例，相信你能从中体会到“决策树”的魅力。古人有“伯乐识别千里马”那么“伯乐”是如何“相马”的呢？下表列出了 A、B、C 、D 四匹马，它们具有以下特征：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164629.png" alt="决策树算法"></p>
<p>如果你是“伯乐”会如何从中挑选出那匹“千里马”呢？毫无疑问，我们要根据马匹的相应特征去判断，而这些特征对应的值叫做“特征维度值”，下面是一位“伯乐”利用 if -else 原理，最终成功的审识别出“千里马”的全过程，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164612.gif" alt="决策树算法"></p>
<p>上图所示是一颗典型的树形结构“二叉树”，而决策树一词中的“树”指的就是这棵树。上图展示了伯乐“识别”千里马的全过程，根据特征值的有无（if-else原理）最终找出“千里马。你可能会问为什么并没囊括所有的特征值？</p>
<p>这是因为某些特征值对于结果的判断而言，并不是最为关键的特征值，比如马的“体型”，“骨瘦如柴”并不能决定某一匹马不是“千里马”。而“马腿”的长短没有作为判断条件，这是因为==使用前三个特征值就已经完成了结果的分类==，如果此时再使用“马腿”长短作为判断条件，则有点多此一举。</p>
<p>如果将上述判断的流程用 if-else 的伪代码写出来，如下所示：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token punctuation">(</span>特征值<span class="token string">"声音"</span>为<span class="token string">"是"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">if</span><span class="token punctuation">(</span>特征值<span class="token string">"眼睛有神"</span>为<span class="token string">"是"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>特征值<span class="token string">"马蹄大"</span>为<span class="token string">"是"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            类别千里马 C

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            类别普通马匹 D

    <span class="token keyword">else</span><span class="token punctuation">:</span>
        类别普通马匹 A

<span class="token keyword">else</span><span class="token punctuation">:</span>
    类别普通马匹 B<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="决策树算法关键"><a href="#决策树算法关键" class="headerlink" title="决策树算法关键"></a>决策树算法关键</h3><p>决策树算法涉及了几个重要的知识点：“决策树的分类方法”，“分支节点划分问题”以及“纯度的概念”。当然在学习过程中还会涉及到“信息熵”、“信息增益”、“基尼指数”的概念，</p>
<h4 id="特征维度-amp-判别条件"><a href="#特征维度-amp-判别条件" class="headerlink" title="特征维度&amp;判别条件"></a>特征维度&amp;判别条件</h4><p>我们知道分类问题的数据集由许多样本构成，而每个样本数据又会有多个特征维度，比如前面例子中马的“声音”，“眼睛”都属于特征维度，在决策算法中这些特征维度属于一个集合，称为“特征维度集”。数据样本的特征维度与最终样本的分类都可能存在着某种关联，因此决策树的<strong>判别条件</strong>将从特征维度集中产生。</p>
<p>在机器学习中，决策树算法是一种有监督的分类算法，我们知道机器学习其实主要完成两件事，一个是模型的训练与测试，另外一个是预测数据的（分类问题，预测类别），因此对于决策树算法而言，我们要考虑如何学会自动选择最合适的判别条件，如图 1 所示，只利用前三个特征就完成了分类的预测。这也将是接下来要探讨的重要问题。</p>
<h2 id="18-决策树算法：选择决策条件"><a href="#18-决策树算法：选择决策条件" class="headerlink" title="18 决策树算法：选择决策条件"></a>18 决策树算法：选择决策条件</h2><blockquote>
<p>首先来看一个“我想你来猜”的游戏，游戏规则很简单：一个人从脑海中构建一个事物，另外几个人最多可以向他提问 20 个问题，游戏规定，问题的答案只能用是或者否来回答。问问题的人通过回答者的“答案”来推分析、逐步缩小待猜测事物的范围，从而来判断他想的是什么。其实这个游戏与决策树工作过程相似。</p>
<p>那么你有没有考虑过要怎样选择“问什么问题”呢，在这里“问什么问题”就相当于决策树算法中的“判别条件”。选择什么判别条件，可以让我们又快又准确的实现分类，这是本节介绍的重点知识。</p>
</blockquote>
<h3 id="纯度的概念"><a href="#纯度的概念" class="headerlink" title="纯度的概念"></a>纯度的概念</h3><p>决策树算法引入了“纯度”的概念，“纯”指的是单一，而“度”则指的是“度量”。“纯度”是对单一类样本在子集内所占重的的度量。</p>
<p>在每一次判别结束后，如果集合中归属于同一类别的样本越多，那么就说明这个集合的纯度就越高。比如，二元分类问题的数据集都会被分成两个子集，我们通过自己的纯度就可以判断分类效果的好与坏，子集的纯度越高，就说明分类效果越好。</p>
<p>上一节我们提到过，决策树算法是一类算法，并非某一种算法，其中最著名的决策树算法有三种，分别是 ID3、C4.5 和 CART。虽然他们都属于决策树算法，不过它们之间也存在着一些细微的差别，主要是体现在衡量“纯度”的方法上，它们分别采用了信息增益、增益率和基尼指数，这些算法的相关概念将在后续内容为大家说明。</p>
<h3 id="纯度度量规则"><a href="#纯度度量规则" class="headerlink" title="纯度度量规则"></a>纯度度量规则</h3><p>要想明确纯度的衡量方法，首先我们要知道一些度量“纯度”的规则。下面我们将类别分为“正类与负类”，如下所示：</p>
<ul>
<li>某个分支节点下所有样本都属于同一个类别，纯度达到最高值。</li>
<li>某个分支节点下样本所属的类别一半是正类一半是负类，此时，纯度取得最低值。</li>
<li>纯度代表一个类在子集中的占比多少，它并不在乎该类究竟是正类还是负类。比如，某个分支下不管是正类占比 60% 还是负类占比 60%，其纯度的度量值都是一样的。</li>
</ul>
<p>决策树算法中使用了大量的二叉树进行判别，在一次判别后，最理想的情况是分支节点下包含的类完全相同，也就是说不同的类别完全分开，但有时我们无法只用一个判别条件就让不同的类之间完全分开，因此选择合适判别条件区划分类是我们要重点掌握的。</p>
<h3 id="纯度度量方法"><a href="#纯度度量方法" class="headerlink" title="纯度度量方法"></a>纯度度量方法</h3><p>根据之前学习的机器学习算法，如果要求得子集内某一类别所占比最大或者最小，就需要使用求极值的方法。因此，接下来探讨使得纯度能够达到最大值和最小值的“纯度函数”。</p>
<h4 id="1-纯度函数"><a href="#1-纯度函数" class="headerlink" title="1) 纯度函数"></a>1) 纯度函数</h4><p>现在我们做一个函数图像，横轴表示某个类的占比，纵轴表示纯度值，然后我们根据上面提出的“纯度度量规则”来绘制函数图像：</p>
<p>首先某个类达到最大值，或者最小值时，纯度达到最高值，然后，当某一个类的占比达到 0.5 时，纯度将取得最低值。由这两个条件，我们可以做出 a/b/c 三个点，最后用一条平滑的曲线将这三个点连接起来。如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164654.gif" alt="纯度函数图像"><br>图1：纯度函数图像</p>
<p>如上图，我们做出了一条类似于抛物线的图像，你可以把它看做成“椭圆”的下半部分。当在 a 点时某一类的占比纯度最小，但是对于二元分类来说，一个类小，另一个类就会高，因此 a 点时的纯度也最高（与 b 恰好相反），当某类的纯度占比在 c 点时，对于二元分类来说，两个类占比相同，此时的纯度值最低，此时通过 c 点无法判断一个子集的所属类别。</p>
<h4 id="2-纯度度量函数"><a href="#2-纯度度量函数" class="headerlink" title="2) 纯度度量函数"></a>2) 纯度度量函数</h4><p>前面在学习线性回归算法时，我们学习了损失函数，它的目的是用来计算损失值，从而调整参数值，使其预测值不断逼近于误差最小，而纯度度量函数的要求正好与纯度函数的要求相反，因为纯度值越低意味着损失值越高，反之则越低。所以纯度度量函数所作出来的图像与纯度函数正好相反。如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164708.gif" alt="纯度度量函数"><br>图2：纯度度量函数</p>
<p>上图就是纯度度量函数，它与纯度函数恰好相反。纯度度量函数图像适应于所有决策树算法，比如 ID3、C4.5、CART 等经典算法。</p>
<h2 id="19-信息熵是什么"><a href="#19-信息熵是什么" class="headerlink" title="19 信息熵是什么"></a>19 信息熵是什么</h2><p>信息是一个很抽象的概念，比如别人说的一段话就包含某些“信息”，或者我们所看到的一个新闻也包含“信息”，</p>
<p>“熵”这一词语从热力学中借用过来的，热力学中的“热熵”是表示分子状态混乱程度的物理量，香农使用“信息熵”这一概念来==量化“信息量”==。信息的计算是非常复杂的，具有多重前提条件的信息，更是无法计算，但由于信息熵和热力熵紧密相关，所以信息熵可以在衰减的过程中被测定出来。</p>
<h3 id="理解信息熵"><a href="#理解信息熵" class="headerlink" title="理解信息熵"></a>理解信息熵</h3><p><code>信息熵是用于衡量不确定性的指标，也就是离散随机事件出现的概率，简单地说“情况越混乱，信息熵就越大，反之则越小”。</code></p>
<blockquote>
<p>全英雄宝箱 和 450金币英雄宝箱 ？</p>
<p>随机性越大，可能性越多，信息熵越大。</p>
</blockquote>
<p>信息熵的计算公式，</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164733.gif" alt="img"></p>
<p>其中 p 代表概率的意思，这里 “X” 表示进行信息熵计算的集合。在决策树分类算法中，我们可以按各个类别的占比（占比越高，该类别纯度越高）来理解，其中 N 表示类别数目，而 Pk 表示类别 K 在子集中的占比。理解了上述含义，再理解信息熵的计算过程就非常简单了，分为三次四则运算，即相乘、求和最后取反。</p>
<h3 id="信息熵公式计算"><a href="#信息熵公式计算" class="headerlink" title="信息熵公式计算"></a>信息熵公式计算</h3><p>在<strong>二元分类问题</strong>中，如果当前样本全部属于 k 类别，那么该类别在子集节点中的占比达到 100%（而另一个类别占比为 0），即 pk = 1，此时信息熵的计算公式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164810.gif" alt="信息熵计算公式"></p>
<p>关于对数函数的运算法则这里不再赘述，以 2 为底 1 的对数为 0，因此最终两个类别的信息熵求和结果为 0。信息熵为 0 说明子集内的类别一致“整齐有序”。由此也可以得知 pk=0.5 时候信息熵的取得最大值。下面根据上述信息，我们绘制<strong>信息熵</strong>的函数图像，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/CCCSL05/PicStore/noteImg/20211229164828.gif" alt="信息熵函数图像"></p>
<h3 id="ID3算法—信息增益"><a href="#ID3算法—信息增益" class="headerlink" title="ID3算法—信息增益"></a>ID3算法—信息增益</h3><h2 id="20-决策树算法和剪枝原理"><a href="#20-决策树算法和剪枝原理" class="headerlink" title="20 决策树算法和剪枝原理"></a>20 决策树算法和剪枝原理</h2><p>我们知道，决策树算法是一种树形分类结构，要通过这棵树实现样本分类，就要根据 if -else 原理设置判别条件。因此您可以这样理解，决策树是由许多 if -else 分枝组合而成的树形模型。</p>
<h3 id="决策树算法原理"><a href="#决策树算法原理" class="headerlink" title="决策树算法原理"></a>决策树算法原理</h3><h3 id="决策树剪枝策略"><a href="#决策树剪枝策略" class="headerlink" title="决策树剪枝策略"></a>决策树剪枝策略</h3><h2 id="21-sklearn决策树分类算法应用"><a href="#21-sklearn决策树分类算法应用" class="headerlink" title="21 sklearn决策树分类算法应用"></a>21 sklearn决策树分类算法应用</h2><h3 id="决策树算法应用"><a href="#决策树算法应用" class="headerlink" title="决策树算法应用"></a>决策树算法应用</h3><p>在 sklearn 库中与决策树相关的算法都存放在<code>sklearn.tree</code>模块里，该模块提供了 4 个决策树算法，下面对这些算法做简单的介绍：</p>
<h4 id="1-DecisionTreeClassifier"><a href="#1-DecisionTreeClassifier" class="headerlink" title="1) .DecisionTreeClassifier()"></a>1) .DecisionTreeClassifier()</h4><p>这是一个经典的决策树分类算法，它提供了许多有用的参数，比如<code>criterion</code>，该参数有两个参数值，分别是 gini（基尼指数）和 entropy（信息增益），默认情况下使用“基尼指数”，其中“gini”用于创建 CART 分类决策树，而“entropy”用于创建 ID3 分类决策树。</p>
<p>注意：在其余三个决策树算法中都可以使用 criterion 参数。</p>
<h4 id="2-DecisionTreeRegressor"><a href="#2-DecisionTreeRegressor" class="headerlink" title="2) .DecisionTreeRegressor()"></a>2) .DecisionTreeRegressor()</h4><p>它表示用决策树算法解决回归问题。</p>
<h4 id="3-ExtraTreeClassifier"><a href="#3-ExtraTreeClassifier" class="headerlink" title="3) .ExtraTreeClassifier()"></a>3) .ExtraTreeClassifier()</h4><p>该算法属于决策树分类算法，但又不同于<code>.DecisionTreeClassifier()</code>算法，因为<code>.ExtraTreeClassifier()</code>选择“特征维度”作为判别条件时具有随机性，它首先从特征集合中随机抽取 n 个特征维度来构建新的集合，然后再从新的集合中选取“判别条件”。</p>
<h4 id="4-ExtraTreeRegressor"><a href="#4-ExtraTreeRegressor" class="headerlink" title="4) .ExtraTreeRegressor()"></a>4) .ExtraTreeRegressor()</h4><p>该算法同样具有随机性，它与<code>.ExtraTreeClassifier()</code>随机过程类似，它主要解决机器学习中的回归问题。</p>
<h3 id="决策树实现步骤"><a href="#决策树实现步骤" class="headerlink" title="决策树实现步骤"></a>决策树实现步骤</h3><p>通过前面内容的学习，我们已经大体掌握了决策树算法的使用流程。决策树分类算法的关键在于选择合适的“判别条件”，该判别条件会使正确的分类的样本“纯度”最高。想要选取合适的特征属性就需要使用“信息熵”与“信息增益”等计算公式。</p>
<h4 id="1-确定纯度指标"><a href="#1-确定纯度指标" class="headerlink" title="1) 确定纯度指标"></a>1) 确定纯度指标</h4><p>确定纯度指标，用它来衡量不同“特征属性”所得到的纯度，并选取使得纯度取得最大值的“特征属性”作为的“判别条件”。</p>
<h4 id="2-切分数据集"><a href="#2-切分数据集" class="headerlink" title="2) 切分数据集"></a>2) 切分数据集</h4><p>通过特征属性做为“判别条件”对数据集集合进行切分。注意，使用过的“特征属性”不允许重复使用，该属性会从特征集合中删除。</p>
<h4 id="3-获取正确分类"><a href="#3-获取正确分类" class="headerlink" title="3) 获取正确分类"></a>3) 获取正确分类</h4><p>选择特征集合内的特征属性，直至没有属性可供选择，或者是数据集样本已经完成分类为止。切记要选择占比最大的类别做为分类结果。</p>
<h3 id="决策树算法应用-1"><a href="#决策树算法应用-1" class="headerlink" title="决策树算法应用"></a>决策树算法应用</h3><p>下面使用决策树算法对 Sklearn 库中的红酒数据进行模型训练，与数据预测，示例代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 加载红酒数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine
<span class="token comment"># 导入决策树分类器</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
<span class="token comment"># 导入分割数据集的方法</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token comment"># 导入科学计算包</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># 加载红酒数据集</span>
wine_dataset<span class="token operator">=</span>load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 分割训练集与测试集</span>
X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># 创建决策时分类器--ID3算法</span>
tree_model<span class="token operator">=</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span><span class="token punctuation">)</span>
<span class="token comment"># 喂入数据</span>
tree_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
<span class="token comment"># 打印模型评分</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tree_model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 给出一组数据预测分类</span>
X_wine_test<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11.8</span><span class="token punctuation">,</span><span class="token number">4.39</span><span class="token punctuation">,</span><span class="token number">2.39</span><span class="token punctuation">,</span><span class="token number">29</span><span class="token punctuation">,</span><span class="token number">82</span><span class="token punctuation">,</span><span class="token number">2.86</span><span class="token punctuation">,</span><span class="token number">3.53</span><span class="token punctuation">,</span><span class="token number">0.21</span><span class="token punctuation">,</span><span class="token number">2.85</span><span class="token punctuation">,</span><span class="token number">2.8</span><span class="token punctuation">,</span><span class="token number">.75</span><span class="token punctuation">,</span><span class="token number">3.78</span><span class="token punctuation">,</span><span class="token number">490</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
predict_result<span class="token operator">=</span>tree_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_wine_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>predict_result<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"分类结果：&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>wine_dataset<span class="token punctuation">[</span><span class="token string">'target_names'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>predict_result<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ACsolin</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://CCCSL05.github.io/posts/a4f8/">https://CCCSL05.github.io/posts/a4f8/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint policy. If reproduced, please indicate source
                    <a href="/about" target="_blank">ACsolin</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                                    <span class="chip bg-color">人工智能</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/posts/aa97/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/47.jpg" class="responsive-img" alt="x100-spring">
                        
                        <span class="card-title">x100-spring</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%8A%80%E6%9C%AF/" class="post-category">
                                    技术
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/spring/">
                        <span class="chip bg-color">spring</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/68a6/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/4.jpg" class="responsive-img" alt="Linux-实训笔记">
                        
                        <span class="card-title">Linux-实训笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%8A%80%E6%9C%AF/" class="post-category">
                                    技术
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Linux/">
                        <span class="chip bg-color">Linux</span>
                    </a>
                    
                    <a href="/tags/%E8%BF%90%E7%BB%B4/">
                        <span class="chip bg-color">运维</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #6756CB;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="3043046905"
                   fixed='true'
                   autoplay='false'
                   theme='#6756CB'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.3'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2015-2021</span>
            
            <a href="/about" target="_blank">ACsolin</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;Total visits:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;Total visitors:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <!-- <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/CCCSL05" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>















    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div> -->
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
